---
title: "Transfer analyses"
output: html_document
---

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
```

# load data
```{r}
num_runs = 500
num_layerss = 2:4
```

```{r message=F, warning=F}
d = data.frame()
for (run_i in 0:(num_runs-1)) {
  for (num_layers in num_layerss) {
    for (analogous in 0:1) {
      for (nonlinear in 0:1) {
        filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_loss_track.csv",
                           num_layers, nonlinear, analogous, run_i)
        if (!file.exists(filename)) {
          next
        }
        this_d = read_csv(filename) %>%
          mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
        d = bind_rows(d, this_d)
      }
    }
  }
}
```

```{r}
d = d %>%
  filter((num_layers==5 & epoch %% 500 == 0) | (num_layers==4 & epoch %% 200 == 0) | (num_layers == 3 & epoch %% 50 == 0) | num_layers == 2) %>%
  complete(run, analogous, nesting(nonlinear, num_layers, epoch), fill=list(MSE=0, d1_MSE=0)) 
```


# basic analysis
```{r}
theme_set(theme_bw() +
          theme(panel.grid=element_blank()))
```

```{r}
ggplot(data=d, aes(x=epoch, y=d1_MSE, color=analogous)) +
  geom_line(stat="summary", fun.y=median, size=1) +
  geom_line(aes(group=interaction(run, analogous)), alpha=0.1) +
  facet_wrap(.~nonlinear + num_layers, scales="free_x") +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

```{r}
ggsave("plots/basic_transfer_comparison.png", width=8, height=4)
```

```{r}
learned_d = d %>%
  filter(d1_MSE < 0.05) %>%
  group_by(nonlinear, analogous, num_layers, run) %>%
  summarize(learned_epoch = min(epoch)) %>%
  ungroup()
```


```{r}
model = lmer(learned_epoch ~ analogous  * nonlinear + (1 | run), learned_d %>% filter(num_layers==3))

summary(model)
```

```{r}
learned_diff_d = learned_d %>%
  spread(analogous, learned_epoch) %>%
  mutate(learn_diff = `non-analogous` - `analogous`)
```

```{r}
ggplot(learned_d %>% filter(nonlinear, num_layers==3), aes(x=learned_epoch, color=analogous)) +
  geom_line(stat="density") +
  facet_grid(. ~ num_layers, scales="free")
```
```{r}
ggsave("plots/basic_transfer_stopping_times.png", width=8, height=4)
```


```{r}
ggplot(learned_diff_d %>% filter(nonlinear, num_layers==3), aes(x=learn_diff)) +
  geom_line(stat="density") +
  facet_grid(. ~ num_layers, scales="free") +
  labs(x="Amount longer it took to learn non-analogous")
```

```{r}
ggsave("plots/basic_transfer_stopping_time_differences.png", width=8, height=4)
```


```{r}
sorted_diff_d = arrange(learned_diff_d %>% filter(num_layers==3, nonlinear), desc(learn_diff))
head(sorted_diff_d)

tail(sorted_diff_d)

```

```{r}
ggplot(data=d %>% filter(run %in% c(80, 104, 107), num_layers==3, nonlinear), aes(x=epoch, y=d1_MSE, color=analogous)) +
  geom_line(aes(group=interaction(run, analogous))) +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

Since we know in the 3 layer case that the transfer effect **must** be happening at the middle layer, since these are the only shared weights. This provides a nice target for analysis.

## why are some 4-layer linear runs showing effects in the other direction?


```{r}
ggplot(data=d %>% filter(run %in% c(98), num_layers==4, !nonlinear), aes(x=epoch, y=d1_MSE, color=analogous)) +
  geom_line(aes(group=interaction(run, analogous))) +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

```{r}
ggplot(data=d %>% filter(run %in% c(98), num_layers==4, !nonlinear), aes(x=epoch, y=MSE, color=analogous)) +
  geom_line(aes(group=interaction(run, analogous))) +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

Might just be single-domain weirdness and the exaggeration that happens with depth... Should investigate further.


# Investigating how the penultimate layer weights change

```{r}
detailed_runs = 0:499
num_layers = 3
```

```{r message=FALSE}
simil_d = data.frame()
sv_d = data.frame()
proj_d = data.frame()
for (run_i in detailed_runs) {
  for (analogous in 0:1) {
    for (nonlinear in c(1)) {
      filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_penultimate_simil_track.csv",
                         num_layers, nonlinear, analogous, run_i)
      if (!file.exists(filename)) {
        next
      }
      this_d = read_csv(filename) %>%
        mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
      simil_d = bind_rows(simil_d, this_d)
      
      filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_penultimate_S_track.csv",
                         num_layers, nonlinear, analogous, run_i)
      this_d = read_csv(filename) %>%
        mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
      sv_d = bind_rows(sv_d, this_d)

      filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_penultimate_proj_track.csv",
                         num_layers, nonlinear, analogous, run_i)
      this_d = read_csv(filename) %>%
        mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
      proj_d = bind_rows(proj_d, this_d)
    }
  }
}
```


## projections
```{r}
part_summarized_proj_d = proj_d %>%
  mutate(mode_rank = mode_j,
         abs_projection=projection) %>%
  select(-projection) %>%
  spread(rep_i, abs_projection) %>%
  group_by(run, analogous, epoch, mode_rank) %>%
  summarize(abs_proj_1 = `0` + `1` + `2`,
            abs_proj_2 = `3` + `4` + `5`,
            abs_proj_diff = abs(abs_proj_1 - abs_proj_2)) %>% # how much is the mode biased towards one domain?
  ungroup() 

summarized_proj_d = part_summarized_proj_d %>%
  group_by(analogous, epoch, mode_rank) %>%
  summarize(mean_abs_proj_diff = mean(abs_proj_diff), sd_abs_proj_diff = sd(abs_proj_diff))

final_proj_d = part_summarized_proj_d %>%
  group_by(run, analogous, mode_rank) %>%
  filter(epoch == max(epoch)) %>%
  ungroup()

early_proj_d = part_summarized_proj_d %>%
  group_by(run, analogous, mode_rank) %>%
  filter(epoch == 5000) %>%
  ungroup() %>%
  select(-abs_proj_1, -abs_proj_2, -epoch) %>%
  rename(early_abs_proj_diff=abs_proj_diff)
  

```

```{r}
ggplot(summarized_proj_d %>% filter(mode_rank < 2), aes(x=epoch, y=mean_abs_proj_diff, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

```{r}
ggplot(final_proj_d, aes(x=abs_proj_diff, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

## similarities 

```{r}
part_summarized_simil_d = simil_d %>%
  rename(cosine_distance=cosine_similarity) %>% # fix incorrect label in data output
  mutate(type= case_when(rep_j == rep_i + 3 ~ "between_analog", 
                         rep_i < 3 & rep_j >= 3 ~ "between_nonanalog",
                         rep_i < 3 & rep_j < 3 ~ "within_d1",
                         T ~ "within_d2"),
         abs_cosine=abs(1-cosine_distance)) %>%
  group_by(run, analogous, epoch, type) %>%
  summarize(mean_abs_cosine = mean(abs_cosine),
            mean_cosine_distance = mean(cosine_distance))

summarized_simil_d = part_summarized_simil_d %>%
  group_by(epoch, analogous, type) %>%
  summarize(mean_abs_cosine = mean(mean_abs_cosine),
            mean_cosine_distance = mean(mean_cosine_distance))

final_simil_d = part_summarized_simil_d %>%
  group_by(run, analogous, type) %>%
  filter(epoch == max(epoch)) %>%
  ungroup()

early_simil_d = part_summarized_simil_d %>%
  group_by(run, analogous, type) %>%
  filter(epoch == 5000) %>%
  ungroup() %>%
  select(-epoch)

initial_simil_d = part_summarized_simil_d %>%
  group_by(run, analogous, type) %>%
  filter(epoch == 0) %>%
  ungroup() %>%
  select(-epoch)

```

```{r}
ggplot(final_simil_d, aes(x=mean_cosine_distance, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~type)
```


```{r}
ggplot(final_simil_d, aes(x=mean_abs_cosine, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~type)
```

## singular values
```{r}
sv_d = sv_d %>%
  rename(mode_rank=rank) %>%
  group_by(run, mode_rank, analogous) %>%
  mutate(norm_S = S/max(S)) %>%
  ungroup()

summarized_sv_d = sv_d %>%
  group_by(analogous, epoch, mode_rank) %>%
  summarize(mean_norm_S=mean(norm_S), mean_S=mean(S))

final_sv_d = sv_d %>%
  group_by(analogous, run, mode_rank) %>% 
  filter(epoch == max(epoch)) %>%
  ungroup()

early_sv_d = sv_d %>%
  group_by(analogous, run, mode_rank) %>% 
  filter(epoch == 5000) %>%
  select(-epoch) %>%
  ungroup()
```

```{r}
ggplot(final_sv_d, aes(x=S, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

```{r}
ggplot(summarized_sv_d, aes(x=epoch, y=mean_norm_S, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_wrap(~mode_rank)
```

```{r}
ggplot(summarized_sv_d, aes(x=epoch, y=mean_S, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

## do these features help explain transfer?
```{r}
learned_and_features_d = learned_d %>%
  filter(nonlinear, num_layers==3) %>%
  left_join(early_sv_d %>%
              filter(mode_rank==0)) %>%
  left_join(final_simil_d %>% 
              select(-mean_abs_cosine) %>%
              mutate(type=paste("cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(early_simil_d %>% 
              select(-mean_abs_cosine) %>%
              mutate(type=paste("early_cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(initial_simil_d %>% 
              select(-mean_abs_cosine) %>%
              mutate(type=paste("initial_cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(final_proj_d %>%
              filter(mode_rank==0)) %>%
  left_join(early_proj_d %>%
              filter(mode_rank==0))
```

```{r}
model_non_analogous = lmer(learned_epoch ~ S + cosine_distance_between_analog + cosine_distance_between_nonanalog + abs_proj_diff + (1 | run), learned_and_features_d)
BIC(model_non_analogous)
summary(model_non_analogous)
```

```{r}
model = lmer(learned_epoch ~ analogous + S + cosine_distance_between_analog + cosine_distance_between_nonanalog + abs_proj_diff + (1 | run), learned_and_features_d)
BIC(model)
summary(model)
```

```{r}
simple_model = lmer(learned_epoch ~ analogous + S + (1 | run), learned_and_features_d)
BIC(simple_model)
summary(simple_model)
```

```{r}
proj_S = lmer(S ~ abs_proj_diff + (1 | run), learned_and_features_d)
summary(proj_S)
```
```{r}
direct = lmer(learned_epoch ~ analogous + (1 | run), learned_and_features_d)
BIC(direct)
summary(direct)
```

Seems like the early increase in S is doing much of the work, but the other features are also important (based on BIC differences).

### actual mediation test

```{r}
library(boot)
```

```{r}
sobel_getter = function(data, indices) {
  boot_data = data[indices,]
  analogous_S = lmer(S ~ analogous + (1 | run), boot_data)
  A = analogous_S@beta[2]
  
  S_learned = lmer(learned_epoch ~ S + (1 | run), boot_data)
  B = S_learned@beta[2]
  
  return(A*B)
}
```

```{r}
set.seed(1)
boot_results = boot(statistic=sobel_getter, data=learned_and_features_d, R=1000)
boot.ci(boot_results)
```

### is the early abs proj diff predictive of early S?

```{r}
early_proj_S = lm(S ~ early_abs_proj_diff, learned_and_features_d)
summary(early_proj_S)
```

No.

### What about similarities?

```{r}
simil_S = lmer(S ~ early_cosine_distance_between_analog + (1|run), learned_and_features_d)
summary(simil_S)
```

```{r}
simil_analog_S = lmer(S ~ early_cosine_distance_between_analog + analogous + (1|run), learned_and_features_d)
summary(simil_analog_S)
```

Is this just because of initialization effects?

```{r}
simil_init_S = lmer(S ~  early_cosine_distance_between_analog + initial_cosine_distance_between_analog + analogous + (1|run), learned_and_features_d)
summary(simil_init_S)
```

No, it appears to really be a key feature (although it's not explaining the full effect, possibly because of time-slicing rather than taking the full trajectory?)

# Can we get more precise about the relationships between these learning features?

```{r}
learning_features_d = d %>%
  filter(nonlinear, num_layers==3, epoch %% 500 == 0) %>%
  left_join(sv_d %>%
              filter(mode_rank==0, epoch %% 500 == 0)) %>%
  filter(!is.na(S)) %>% # the other datasets haven't been completed/we don't want the predictivity estimates to be skewed.
  left_join(part_summarized_simil_d %>% 
              filter(epoch %% 500 == 0) %>%
              select(-mean_abs_cosine) %>%
              mutate(type=paste("cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(part_summarized_proj_d %>%
              filter(mode_rank==0, epoch %% 500 == 0)) %>%
  group_by(run, analogous) %>%
  mutate(S_z = scale(S),
         norm_S_z = scale(norm_S),
         abs_proj_diff_z = scale(abs_proj_diff),
         cosine_distance_between_analog_z = scale(cosine_distance_between_analog),
         d1_MSE_z = scale(d1_MSE)) %>%
  ungroup()
```

```{r}
lag_vars = function(vars_to_lag, lags, grouping_vars, data) {
  res_data = data %>% 
    gather(variable, value, one_of(vars_to_lag)) %>%
    group_by_at(vars(one_of(c("variable", grouping_vars))))
  for (l in lags) {
    res_data = res_data %>%
      mutate(!!(sprintf("lag_%i", l)) := lag(value, l))
  }
  res_data = res_data %>%
    ungroup() %>%
    rename(lag_none=value) %>%
    gather(lag, value, starts_with("lag")) %>%
    unite(var_and_lag, variable, lag) %>%
    mutate(var_and_lag = str_replace(var_and_lag, "_lag_none", "")) %>%
    spread(var_and_lag, value)
    
  return(res_data)
}
```

```{r}
predictivity_getter = function(var_A, var_B, grouping_vars, lag, data) {
  lagged_var_B = sprintf("%s_lag", var_B)
  formula = sprintf("%s ~ %s + (1|run)", var_A, lagged_var_B, lag) # there are issues with the choice of including a random effect of run
                                                            # but the issues with not including it seem worse, since the predictor
                                                            # of real interest (analogous) is independent of run.
  this_data = data %>% # this awkward step prevents lagging across run boundaries
    group_by_at(vars(one_of(grouping_vars))) %>%
    mutate(!!lagged_var_B := lag(!!sym(var_B), lag)) %>%
    ungroup()
  res = lmer(formula, this_data)
  return(res@beta[2])
}
predictivity_getter("S_z", "cosine_distance_between_analog_z", c("run", "analogous"), 1, learning_features_d)
```

```{r testing functions, eval=F}
dummy_d = data.frame(blah=rep(c("A", "B"), each=5),
                     time=rep(1:5, 2),
                     x=rnorm(10)) %>%
  group_by(blah) %>%
  mutate(y = ifelse(is.na(lag(x, 1)), rnorm(1), lag(x, 1) + rnorm(1, sd=0.1))) %>%
  ungroup()
dummy_d
lag_vars(c("x","y"), 0:2, c("blah"), dummy_d)
```
