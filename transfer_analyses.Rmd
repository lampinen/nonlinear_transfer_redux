---
title: "Transfer analyses"
output: html_document
---

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
```

# load data
```{r}
num_runs = 500
num_layerss = c(3)
```

```{r message=F, warning=F}
d = data.frame()
for (run_i in 0:(num_runs-1)) {
  for (num_layers in num_layerss) {
    for (analogous in 0:1) {
      for (nonlinear in c(1)) {
        filename = sprintf("eight_things_results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_loss_track.csv",
                           num_layers, nonlinear, analogous, run_i)
        if (!file.exists(filename)) {
          next
        }
        this_d = read_csv(filename) %>%
          mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
        d = bind_rows(d, this_d)
      }
    }
  }
}
```

```{r}
d = d %>%
  filter((num_layers==5 & epoch %% 500 == 0) | (num_layers==4 & epoch %% 200 == 0) | (num_layers == 3 & epoch %% 10 == 0) | num_layers == 2) %>%
  complete(run, analogous, nesting(nonlinear, num_layers, epoch), fill=list(MSE=0, d1_MSE=0)) 
```


# basic analysis
```{r}
theme_set(theme_bw() +
          theme(panel.grid=element_blank()))
```

```{r}
ggplot(data=d, aes(x=epoch, y=d1_MSE, color=analogous)) +
  geom_line(stat="summary", fun.y=median, size=1) +
  geom_line(aes(group=interaction(run, analogous)), alpha=0.1) +
  facet_wrap(.~nonlinear + num_layers, scales="free_x") +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

```{r}
ggsave("plots/eight_things_transfer_comparison.png", width=8, height=4)
```

```{r}
learned_d = d %>%
  filter(d1_MSE < 0.05) %>%
  group_by(nonlinear, analogous, num_layers, run) %>%
  summarize(learned_epoch = min(epoch)) %>%
  ungroup()
```


```{r}
model = lmer(learned_epoch ~ analogous  + (1 | run), learned_d %>% filter(num_layers==3))

summary(model)
```

```{r}
learned_diff_d = learned_d %>%
  spread(analogous, learned_epoch) %>%
  mutate(learn_diff = `non-analogous` - `analogous`)
```

```{r}
ggplot(learned_d %>% filter(nonlinear, num_layers==3), aes(x=learned_epoch, color=analogous)) +
  geom_line(stat="density", adjust=1) +
  facet_grid(. ~ num_layers, scales="free")
```
```{r}
ggsave("plots/et_transfer_stopping_times.png", width=8, height=4)
```


```{r}
ggplot(learned_diff_d %>% filter(nonlinear, num_layers==3), aes(x=learn_diff)) +
  geom_line(stat="density") +
  facet_grid(. ~ num_layers, scales="free") +
  labs(x="Amount longer it took to learn non-analogous")
```

```{r}
ggsave("plots/et_transfer_stopping_time_differences.png", width=8, height=4)
```


```{r}
sorted_diff_d = arrange(learned_diff_d %>% filter(num_layers==3, nonlinear), desc(learn_diff))
head(sorted_diff_d)

tail(sorted_diff_d)

```

```{r}
ggplot(data=d %>% filter(run %in% c(80, 104, 107), num_layers==3, nonlinear), aes(x=epoch, y=d1_MSE, color=analogous)) +
  geom_line(aes(group=interaction(run, analogous))) +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

Since we know in the 3 layer case that the transfer effect **must** be happening at the middle layer, since these are the only shared weights. This provides a nice target for analysis.

## why are some 4-layer linear runs showing effects in the other direction?


```{r}
ggplot(data=d %>% filter(run %in% c(98), num_layers==4, !nonlinear), aes(x=epoch, y=d1_MSE, color=analogous)) +
  geom_line(aes(group=interaction(run, analogous))) +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

```{r}
ggplot(data=d %>% filter(run %in% c(98), num_layers==4, !nonlinear), aes(x=epoch, y=MSE, color=analogous)) +
  geom_line(aes(group=interaction(run, analogous))) +
  labs(y="Loss (L2)") +
  scale_color_brewer(palette="Set1")
```

Might just be single-domain weirdness and the exaggeration that happens with depth... Should investigate further.


# Investigating how the penultimate layer weights change

```{r}
detailed_runs = 0:499
num_layers = 3
```

```{r message=FALSE}
simil_d = data.frame()
sv_d = data.frame()
proj_d = data.frame()
for (run_i in detailed_runs) {
  for (analogous in 0:1) {
    for (nonlinear in c(1)) {
      filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_penultimate_simil_track.csv",
                         num_layers, nonlinear, analogous, run_i)
      if (!file.exists(filename)) {
        next
      }
      this_d = read_csv(filename) %>%
        mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
      simil_d = bind_rows(simil_d, this_d)
      
      filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_penultimate_S_track.csv",
                         num_layers, nonlinear, analogous, run_i)
      this_d = read_csv(filename) %>%
        mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
      sv_d = bind_rows(sv_d, this_d)

      filename = sprintf("results/nlayer_%i_nonlinear_%i_analogous_%i_rseed_%i_penultimate_proj_track.csv",
                         num_layers, nonlinear, analogous, run_i)
      this_d = read_csv(filename) %>%
        mutate(run=run_i, nonlinear=nonlinear==1, analogous=factor(analogous, levels=0:1, labels=c("non-analogous", "analogous")), num_layers=num_layers)
      proj_d = bind_rows(proj_d, this_d)
    }
  }
}
```


## projections
```{r}
part_summarized_proj_d = proj_d %>%
  mutate(mode_rank = mode_j,
         abs_proj=abs(projection)) %>%
  gather(measurement, value, projection, abs_proj) %>%
  unite(type_and_index, measurement, rep_i) %>%
  spread(type_and_index, value) %>%
  group_by(run, analogous, epoch, mode_rank) %>%
  summarize(d1_abs_proj = abs_proj_0 + abs_proj_1 + abs_proj_2,
            d2_abs_proj = abs_proj_3 + abs_proj_4 + abs_proj_5,
            abs_proj_diff = abs(d1_abs_proj - d2_abs_proj),# how much is the mode biased towards one domain?
            proj_dot = projection_0 * projection_3 + projection_1 * projection_4 + projection_2 * projection_5, # an un-normalized inner product
            abs_proj_dot = abs(proj_dot)) %>% 
  ungroup() 

summarized_proj_d = part_summarized_proj_d %>%
  group_by(analogous, epoch, mode_rank) %>%
  summarize(mean_abs_proj_diff = mean(abs_proj_diff), sd_abs_proj_diff = sd(abs_proj_diff),
            mean_proj_dot = mean(proj_dot),
            mean_abs_proj_dot = mean(abs_proj_dot))

final_proj_d = part_summarized_proj_d %>%
  group_by(run, analogous, mode_rank) %>%
  filter(epoch == max(epoch)) %>%
  ungroup()

early_proj_d = part_summarized_proj_d %>%
  filter(epoch == 5000) %>%
  select(-d1_abs_proj, -d2_abs_proj, -epoch) %>%
  rename(early_abs_proj_diff=abs_proj_diff, early_proj_dot=proj_dot,
         early_abs_proj_dot=abs_proj_dot)
  

```

```{r}
ggplot(summarized_proj_d %>% filter(mode_rank < 2), aes(x=epoch, y=mean_abs_proj_diff, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

```{r}
ggplot(final_proj_d, aes(x=abs_proj_diff, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

```{r}
ggplot(summarized_proj_d %>% filter(mode_rank < 2), aes(x=epoch, y=mean_proj_dot, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```


```{r}
ggplot(summarized_proj_d %>% filter(mode_rank < 2), aes(x=epoch, y=log(mean_abs_proj_dot), color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

```{r}
ggplot(final_proj_d, aes(x=log(abs_proj_dot), color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

## similarities 

```{r}
part_summarized_simil_d = simil_d %>%
  rename(cosine_distance=cosine_similarity) %>% # fix incorrect label in data output
  mutate(type= case_when(rep_j == rep_i + 3 ~ "between_analog", 
                         rep_i < 3 & rep_j >= 3 ~ "between_nonanalog",
                         rep_i < 3 & rep_j < 3 ~ "within_d1",
                         T ~ "within_d2"),
         abs_cosine=abs(1-cosine_distance)) %>%
  group_by(run, analogous, epoch, type) %>%
  summarize(mean_abs_cosine = mean(abs_cosine),
            mean_cosine_distance = mean(cosine_distance))

summarized_simil_d = part_summarized_simil_d %>%
  group_by(epoch, analogous, type) %>%
  summarize(mean_abs_cosine = mean(mean_abs_cosine),
            mean_cosine_distance = mean(mean_cosine_distance))

final_simil_d = part_summarized_simil_d %>%
  group_by(run, analogous, type) %>%
  filter(epoch == max(epoch)) %>%
  ungroup()

early_simil_d = part_summarized_simil_d %>%
  group_by(run, analogous, type) %>%
  filter(epoch == 5000) %>%
  ungroup() %>%
  select(-epoch)

initial_simil_d = part_summarized_simil_d %>%
  group_by(run, analogous, type) %>%
  filter(epoch == 0) %>%
  ungroup() %>%
  select(-epoch)

```

```{r}
ggplot(final_simil_d, aes(x=mean_cosine_distance, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~type)
```


```{r}
ggplot(final_simil_d, aes(x=mean_abs_cosine, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~type)
```

## singular values
```{r}
sv_d = sv_d %>%
  rename(mode_rank=rank) %>%
  group_by(run, mode_rank, analogous) %>%
  mutate(norm_S = S/max(S)) %>%
  ungroup()

summarized_sv_d = sv_d %>%
  group_by(analogous, epoch, mode_rank) %>%
  summarize(mean_norm_S=mean(norm_S), mean_S=mean(S))

final_sv_d = sv_d %>%
  group_by(analogous, run, mode_rank) %>% 
  filter(epoch == max(epoch)) %>%
  ungroup()

early_sv_d = sv_d %>%
  group_by(analogous, run, mode_rank) %>% 
  filter(epoch == 5000) %>%
  select(-epoch) %>%
  ungroup()
```

```{r}
ggplot(final_sv_d, aes(x=S, color=analogous)) +
  geom_density() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

```{r}
ggplot(summarized_sv_d, aes(x=epoch, y=mean_norm_S, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_wrap(~mode_rank)
```

```{r}
ggplot(summarized_sv_d, aes(x=epoch, y=mean_S, color=analogous)) +
  geom_line() +
  scale_color_brewer(palette="Set1") +
  facet_grid(~mode_rank)
```

## do these features help explain transfer?
```{r}
learned_and_features_d = learned_d %>%
  filter(nonlinear, num_layers==3) %>%
  left_join(early_sv_d %>%
              filter(mode_rank==0)) %>%
  left_join(final_simil_d %>% 
              select(-mean_abs_cosine) %>%
              mutate(type=paste("cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(early_simil_d %>% 
              select(-mean_abs_cosine) %>%
              mutate(type=paste("early_cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(initial_simil_d %>% 
              select(-mean_abs_cosine) %>%
              mutate(type=paste("initial_cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(final_proj_d %>%
              filter(mode_rank==0)) %>%
  left_join(early_proj_d %>%
              filter(mode_rank==0))
```

```{r}
model_non_analogous = lmer(learned_epoch ~ S + cosine_distance_between_analog + cosine_distance_between_nonanalog + abs_proj_diff + (1 | run), learned_and_features_d)
BIC(model_non_analogous)
summary(model_non_analogous)
```

```{r}
model = lmer(learned_epoch ~ analogous + S + cosine_distance_between_analog + cosine_distance_between_nonanalog + abs_proj_diff + (1 | run), learned_and_features_d)
BIC(model)
summary(model)
```

```{r}
simple_model = lmer(learned_epoch ~ analogous + S + (1 | run), learned_and_features_d)
BIC(simple_model)
summary(simple_model)
```

```{r}
proj_S = lmer(S ~ abs_proj_diff + (1 | run), learned_and_features_d)
summary(proj_S)
```
```{r}
direct = lmer(learned_epoch ~ analogous + (1 | run), learned_and_features_d)
BIC(direct)
summary(direct)
```

Seems like the early increase in S is doing much of the work, but the other features are also important (based on BIC differences).

### actual mediation test

```{r}
library(boot)
```

```{r}
sobel_getter = function(data, indices) {
  boot_data = data[indices,]
  analogous_S = lmer(S ~ analogous + (1 | run), boot_data)
  A = analogous_S@beta[2]
  
  S_learned = lmer(learned_epoch ~ S + (1 | run), boot_data)
  B = S_learned@beta[2]
  
  return(A*B)
}
```

```{r}
set.seed(1)
boot_results = boot(statistic=sobel_getter, data=learned_and_features_d, R=1000)
boot.ci(boot_results)
```

### is the early abs proj diff predictive of early S?

```{r}
early_proj_S = lm(S ~ early_abs_proj_diff, learned_and_features_d)
summary(early_proj_S)
```

No.

### What about similarities?

```{r}
simil_S = lmer(S ~ early_cosine_distance_between_analog + (1|run), learned_and_features_d)
summary(simil_S)
```

```{r}
simil_analog_S = lmer(S ~ early_cosine_distance_between_analog + analogous + (1|run), learned_and_features_d)
summary(simil_analog_S)
```

Is this just because of initialization effects?

```{r}
simil_init_S = lmer(S ~  early_cosine_distance_between_analog + initial_cosine_distance_between_analog + analogous + (1|run), learned_and_features_d)
summary(simil_init_S)
```

No, it appears to really be a key feature, although actually it's not explaining much of the analogy effect at all.

```{r}
analogy_S = lmer(S ~  analogous + (1|run), learned_and_features_d)
summary(analogy_S)
```

# Can we get more precise about the relationships between these learning features?

```{r}
learning_features_d = d %>%
  filter(nonlinear, num_layers==3, epoch %% 500 == 0) %>%
  left_join(sv_d %>%
              filter(mode_rank==0, epoch %% 500 == 0)) %>%
  filter(!is.na(S)) %>% # the other datasets haven't been completed/we don't want the predictivity estimates to be skewed.
  left_join(part_summarized_simil_d %>% 
              filter(epoch %% 500 == 0) %>%
              select(-mean_abs_cosine) %>%
              mutate(type=paste("cosine_distance_", type, sep="")) %>%
              spread(type, mean_cosine_distance)) %>%
  left_join(part_summarized_proj_d %>%
              filter(mode_rank==0, epoch %% 500 == 0)) %>%
#  group_by(run, analogous) %>%
  mutate(S_z = scale(S),
         norm_S_z = scale(norm_S),
         abs_proj_diff_z = scale(abs_proj_diff),
         log_abs_proj_dot_z = scale(log(abs_proj_dot)), 
         cosine_distance_between_analog_z = scale(cosine_distance_between_analog),
         cosine_distance_between_nonanalog_z = scale(cosine_distance_between_nonanalog),
         cosine_distance_within_d1_z = scale(cosine_distance_within_d1),
         d1_MSE_z = scale(d1_MSE))# %>%
#  ungroup()
```
Lag variables within the data
```{r}
lag_vars = function(vars_to_lag, lags, grouping_vars, data) {
  res_data = data %>% 
    gather(variable, value, one_of(vars_to_lag)) %>%
    group_by_at(vars(one_of(c("variable", grouping_vars))))
  for (l in lags) {
    res_data = res_data %>%
      mutate(!!(sprintf("lag_%i", l)) := lag(value, l))
  }
  res_data = res_data %>%
    ungroup() %>%
    rename(lag_none=value) %>%
    gather(lag, value, starts_with("lag")) %>%
    unite(var_and_lag, variable, lag) %>%
    mutate(var_and_lag = str_replace(var_and_lag, "_lag_none", "")) %>%
    spread(var_and_lag, value)
    
  return(res_data)
}
```

```{r}
predictivity_getter = function(var_Y, var_X, data) {
  formula = sprintf("%s ~ %s + (1|run)", var_Y, var_X) # there are issues with the choice of including a random effect of run
                                                       # but the issues with not including it seem worse, since the predictor
                                                       # of real interest (analogous) is independent of run.
  res = lmer(formula, data)
  return(res@beta[2])
  
  # formula = sprintf("%s ~ %s", var_Y, var_X)
  # res = lm(formula, data)
  # return(res$coefficients[2])
}
```

Generate plots
```{r}
generate_lagged_predictivity_plots = function(vars_of_interest, lags, grouping_vars, data) {
  data = lag_vars(vars_of_interest, lags, grouping_vars, data)  
  plot_d = data.frame()
  for (var_Y in vars_of_interest) {
    for (var_X in vars_of_interest) {
      for (l in lags) {
        var_X_lagged = sprintf("%s_lag_%i", var_X, l)
        this_d = data.frame(var_Y=var_Y, var_X=var_X, lag=l,
                            predictivity=predictivity_getter(var_Y,
                                                             var_X_lagged,
                                                             data))
        plot_d = bind_rows(plot_d, this_d)
      }
    }
  }
  
  p = ggplot(plot_d, aes(x=lag, y=predictivity)) +
    geom_line() +
    geom_point() +
    scale_x_continuous(breaks=lags) +
    scale_y_continuous(breaks=-1:1) +
    ylim(-1.1, 1.1) +
    geom_hline(yintercept = 0, alpha=0.2, linetype=2) +
    geom_hline(yintercept = 1, alpha=0.2, linetype=2) +
    geom_hline(yintercept = -1, alpha=0.2, linetype=2) +
    facet_grid(var_X ~ var_Y)
  return(p)
}
```

```{r testing functions, eval=F}
dummy_d = data.frame(run=rep(c("A", "B"), each=10),
                     time=rep(1:10, 2),
                     x=rnorm(20)) %>%
  group_by(run) %>%
  mutate(y = ifelse(is.na(lag(x, 1)), rnorm(1), lag(x, 1) + rnorm(n(), sd=0.1))) %>%
  ungroup()
dummy_d
dummy_d_lagged = lag_vars(c("x","y"), 0:2, c("run"), dummy_d)
predictivity_getter("y", "x", dummy_d_lagged)
predictivity_getter("y", "x_lag_1", dummy_d_lagged)
generate_lagged_predictivity_plots(c("y","x"), 0:2, c("run"), dummy_d)
```
```{r}
vars_of_interest = c("d1_MSE_z", "S_z", "log_abs_proj_dot_z", "cosine_distance_between_analog_z", "cosine_distance_between_nonanalog_z", "cosine_distance_within_d1_z")
grouping_vars = c("run", "analogous")
lags = 0:10

p = generate_lagged_predictivity_plots(vars_of_interest, lags, grouping_vars, learning_features_d)
p + 
  labs(x="Lag (*500 epochs)", y="Correlation")
```
```{r}
ggsave("plots/time_correlation_predictivity.png", width=12, height=9)
```



